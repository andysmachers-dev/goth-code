# -*- coding: utf-8 -*-
"""Gen AI LAB 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14bqSAmfaglPivCqszG5GIAzoUflR8rb-

20230802111 NIKHIL CHAPKANADE
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed

MODEL_NAME = "gpt2" # or "gpt2-medium", "gpt2-large", "gpt2-xl"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def load_model(model_name=MODEL_NAME, device=DEVICE):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # For GPT-2 ensure tokenizer has pad token (some older tokenizers don't)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(model_name)
    model.to(device)
    model.eval()
    return tokenizer, model

def generate(prompt: str, tokenizer=None, model=None,
             max_length: int = 100,
             num_return_sequences: int = 1,
             temperature: float = 1.0,
             top_k: int = 50,
             top_p: float = 0.95,
             seed: int = 42):

    set_seed(seed)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_length=max_length,
            do_sample=True,
            top_k=top_k,
            top_p=top_p,
            temperature=temperature,
            num_return_sequences=num_return_sequences,
            pad_token_id=tokenizer.eos_token_id
        )

    return [tokenizer.decode(o, skip_special_tokens=True) for o in out]

if __name__ == "__main__":
    tokenizer, model = load_model()
    prompt = "In the near future, artificial intelligence will"
    outputs = generate(prompt, tokenizer=tokenizer, model=model,
                     max_length=120, num_return_sequences=3, temperature=0.9)

    for i, text in enumerate(outputs, 1):
        print(f"--- Generated {i} ---\n{text}\n")





        #### lab 2


  import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

train_loader = DataLoader(
    datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor()),
    batch_size=128, shuffle=True
)

test_loader = DataLoader(
    datasets.MNIST('./data', train=False, transform=transforms.ToTensor()),
    batch_size=128
)

class VAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 200)
        self.fc_mu = nn.Linear(200, 20)
        self.fc_logvar = nn.Linear(200, 20)
        self.fc2 = nn.Linear(20, 200)
        self.fc3 = nn.Linear(200, 28*28)

    def forward(self, x):
        # encoder
        h = F.relu(self.fc1(x.view(-1, 28*28)))
        mu, logvar = self.fc_mu(h), self.fc_logvar(h)
        # reparameterization
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        # decoder
        h2 = F.relu(self.fc2(z))
        recon = torch.sigmoid(self.fc3(h2))
        return recon, mu, logvar


# ---- Create Model ----
model = VAE()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# ---- Training ----
for epoch in range(3): # train only 3 epochs
    for data, _ in train_loader:
        recon, mu, logvar = model(data)

        # compute loss (reconstruction + KL)
        bce = F.binary_cross_entropy(recon, data.view(-1, 28*28), reduction='sum')
        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        loss = bce + kld

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print("Epoch", epoch+1, "Loss:", loss.item())

# ---- Test Reconstruction ----
data, _ = next(iter(test_loader))
recon, _ , _ = model(data)

fig, axes = plt.subplots(2, 8, figsize=(10,3))
for i in range(8):
    axes[0, i].imshow(data[i].squeeze(), cmap='gray') # original
    axes[0, i].axis('off')
    axes[1, i].imshow(recon[i].view(28,28).detach().numpy(), cmap='gray') # reconstruction
    axes[1, i].axis('off')
plt.show()



### lab 3




# -*- coding: utf-8 -*-
"""GenAILab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LW746s9u15UtKTE_insquy14k66GhRGP
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# --- 1. Data (Fashion-MNIST) ---
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)) # Normalizes images to [-1, 1]
])
# ONLY CHANGE IS HERE: datasets.MNIST -> datasets.FashionMNIST
data = datasets.FashionMNIST(root="./data", train=True, download=True, transform=transform)
loader = DataLoader(data, batch_size=64, shuffle=True)

# --- 2. Model Definitions (No changes needed) ---
# Generator
G = nn.Sequential(
    nn.Linear(100, 256),
    nn.ReLU(),
    nn.Linear(256, 512),
    nn.ReLU(),
    nn.Linear(512, 784), # 784 = 28 * 28 * 1
    nn.Tanh()
)

# Discriminator
D = nn.Sequential(
    nn.Linear(784, 512),
    nn.LeakyReLU(0.2),
    nn.Linear(512, 256),
    nn.LeakyReLU(0.2),
    nn.Linear(256, 1),
    nn.Sigmoid()
)

# --- 3. Loss + Optimizers ---
loss_fn = nn.BCELoss()
opt_G = optim.Adam(G.parameters(), lr=0.0002)
opt_D = optim.Adam(D.parameters(), lr=0.0002)

fixed_z = torch.randn(16, 100)

# --- 4. Training Loop ---
epochs = 30
print(f"üöÄ Starting training on Fashion-MNIST for {epochs} epochs...")

for epoch in range(epochs):
    for imgs, _ in loader:
        real = imgs.view(-1, 784)
        real_labels = torch.ones(real.size(0), 1)

        z = torch.randn(real.size(0), 100)
        fake = G(z)
        fake_labels = torch.zeros(real.size(0), 1)

        # Train Discriminator
        d_loss_real = loss_fn(D(real), real_labels)
        d_loss_fake = loss_fn(D(fake.detach()), fake_labels)
        d_loss = d_loss_real + d_loss_fake

        opt_D.zero_grad()
        d_loss.backward()
        opt_D.step()

        # Train Generator
        g_loss = loss_fn(D(fake), real_labels)

        opt_G.zero_grad()
        g_loss.backward()
        opt_G.step()

    print(f"Epoch [{epoch+1}/{epochs}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}")

    # --- 5. Visualization (every 5 epochs) ---
    if (epoch + 1) % 5 == 0:
        with torch.no_grad():
            samples = G(fixed_z).view(-1, 1, 28, 28)
            fig, axes = plt.subplots(4, 4, figsize=(8, 8))

            for i, ax in enumerate(axes.flatten()):
                img_data = samples[i][0] * 0.5 + 0.5
                ax.imshow(img_data, cmap="gray")
                ax.axis("off")

            plt.suptitle(f"Generated Images after Epoch {epoch+1}", fontsize=16)
            plt.show()

print("‚úÖ Training complete!")



###LAB 4


# --- BLOCK 1: INSTALL & SETUP ---
!pip -q install torch torchvision diffusers transformers accelerate safetensors huggingface_hub

import torch
from diffusers import StableDiffusionPipeline
from huggingface_hub import login
from google.colab import userdata
from IPython.display import display


try:
    token = userdata.get('HF_TOKEN')
    login(token=token)
    print("‚úÖ Logged in successfully!")
except Exception as e:
    print("‚ö†Ô∏è Warning: Not logged in. (This might still work if the model is public)")


print("‚è≥ Loading Stable Diffusion v1-5 Mirror... (This takes 1-2 mins)")
model_id = "stable-diffusion-v1-5/stable-diffusion-v1-5"

pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    variant="fp16"
).to("cuda")

print("‚úÖ Model loaded!")

# --- BLOCK 3: GENERATE ---
prompt = "a cute robot painting a canvas, futuristic art studio, 8k, highly detailed, cinematic lighting"
negative_prompt = "blurry, low quality, ugly, watermark, text"

print(f"üé® Generating: '{prompt}'...")
image = pipe(
    prompt,
    negative_prompt=negative_prompt,
    guidance_scale=7.5,
    num_inference_steps=30
).images[0]

display(image)
image.save("robot_art.png")






### lab 5


# --- STEP 1: INSTALL ---
# Install libraries for local text generation
!pip install -q transformers sentencepiece accelerate

from transformers import pipeline
import torch

# --- STEP 2: LOAD MODEL (Local/Free) ---
# We use Google's Flan-T5 (Great for following instructions)
print("‚è≥ Loading Model... (approx 1-2 mins)")
generator = pipeline(
    "text2text-generation",
    model="google/flan-t5-base",
    device_map="auto",  # Uses GPU if available
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
)
print("‚úÖ Model Loaded!")

def ask(prompt):
    # Generates text based on the input prompt
    return generator(prompt, max_length=200)[0]['generated_text']

# --- STEP 3: DEMONSTRATE PROMPT ENGINEERING ---
prompts = {
    "Zero-Shot": "Translate 'How are you?' to French",
    "Few-Shot": "English: Hello -> French: Bonjour\nEnglish: Good morning -> French: Bonjour\nEnglish: Thank you ->",
    "Chain-of-Thought": "A farmer has 12 apples, gives 5 away, eats 2. How many left? Explain step by step.",
    "Role Prompting": "You are a math tutor. Explain Pythagoras theorem simply.",
    "Style Prompting": "Explain Artificial Intelligence like a pirate.",
    "Constraints": "Summarize Machine Learning in exactly 3 words."
}

# Run through all examples
for name, text in prompts.items():
    print(f"\nüîπ {name}:")
    print(f"   Input: {text}")
    print(f"   Output: {ask(text)}")



### lab 6
!pip install -q transformers datasets peft accelerate

import os
os.environ["WANDB_DISABLED"] = "true"
import torch # Added for the 'device' variable

from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import Dataset
from peft import LoraConfig, get_peft_model, TaskType
# 'from google.colab import files' is no longer needed

# --- 1. Load model & tokenizer ---
tok = AutoTokenizer.from_pretrained("gpt2")
tok.pad_token = tok.eos_token
model = AutoModelForCausalLM.from_pretrained("gpt2")
model.resize_token_embeddings(len(tok))

# --- 2. Prepare dataset from file path ---
# No more upload prompt. We use the path you provided.
file_path = "/content/chat.txt"
data = None

try:
    text = open(file_path).read().splitlines()
    if not text:
        print(f"ERROR: The file at {file_path} is empty.")
    else:
        data = Dataset.from_dict({"text": text}).map(lambda e: tok(e["text"], truncation=True, padding="max_length", max_length=128), batched=True)
        print(f"Successfully loaded {len(text)} lines from {file_path}")
except FileNotFoundError:
    print(f"ERROR: File not found at {file_path}")
    print("Please run the '%%writefile' cell above to create 'chat.txt' first.")
except Exception as e:
    print(f"An error occurred: {e}")


# Only proceed if data was loaded successfully
if data:
    # --- 3. Apply LORA ---
    model = get_peft_model(model, LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=32, lora_dropout=0.1))

    # --- 4. Train ---
    trainer = Trainer(
        model=model,
        args=TrainingArguments(
            output_dir="./lora-llm",
            per_device_train_batch_size=2,
            num_train_epochs=1,
            learning_rate=2e-4
        ),
        train_dataset=data,
        data_collator=DataCollatorForLanguageModeling(tok, mlm=False)
    )

    print("--- Starting training... ---")
    trainer.train()
    print("--- Training complete. ---")


    # --- 5. Test ---
    print("\n--- Generating test output... ---")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    prompt_text = "Many human mental activities such as developing computer programs, working out mathematics, engaging in common sense reasoning, understanding languages and interpreting it"
    inputs = tok(prompt_text, return_tensors="pt").to(device)

    out = model.generate(**inputs, max_length=100)
    print(tok.decode(out[0], skip_special_tokens=True))

else:
    print("\nTraining and testing skipped because data was not loaded.")



    ### lab 7


    import numpy as np


# STEP 1: MAKE FAKE ‚ÄúREAL‚Äù DATA + TWO MODELS' FAKE DATA

real = np.random.normal(0, 1, (100, 64))      # real features
fake_A = np.random.normal(0.2, 1, (100, 64))  # model A
fake_B = np.random.normal(-0.2, 1, (100, 64)) # model B

# STEP 3: FID (SIMULATED)
def compute_fid(real, fake):
    mu_r, mu_f = real.mean(axis=0), fake.mean(axis=0)
    cov_r, cov_f = np.cov(real, rowvar=False), np.cov(fake, rowvar=False)

    # simplified FID (no matrix sqrt to keep it beginner-friendly)
    return np.sum((mu_r - mu_f)**2) + np.trace(cov_r + cov_f - 2*np.sqrt(cov_r * cov_f))



# STEP 3: SIMPLE INCEPTION SCORE (SIMULATED)

def compute_is(fake):
    # softmax-like probabilities
    p_yx = np.abs(fake) / np.sum(np.abs(fake), axis=1, keepdims=True)
    p_y = p_yx.mean(axis=0)

    # KL divergence mean
    kl = np.sum(p_yx * (np.log(p_yx + 1e-8) - np.log(p_y + 1e-8)), axis=1)
    return np.exp(kl.mean())



# STEP 4: SIMPLE PRECISION‚ÄìRECALL FOR DISTRIBUTIONS (PRD)

def compute_prd(real, fake):
    # distance to nearest neighbor (VERY simple version)
    pr = np.mean(np.linalg.norm(fake - real.mean(axis=0), axis=1))
    rc = np.mean(np.linalg.norm(real - fake.mean(axis=0), axis=1))

    # invert so higher = better
    precision = 1 / (1 + pr)
    recall = 1 / (1 + rc)

    return precision, recall

     #STEP 5: CALCULATE METRICS FOR MODEL A AND B

fid_A = compute_fid(real, fake_A)
is_A = compute_is(fake_A)
p_A, r_A = compute_prd(real, fake_A)

fid_B = compute_fid(real, fake_B)
is_B = compute_is(fake_B)
p_B, r_B = compute_prd(real, fake_B)


# STEP 6: PRINT RESULTS

print("=============== MODEL COMPARISON ===============")
print(f"Model A ‚Üí FID: {fid_A:.2f},  IS: {is_A:.2f},  Precision: {p_A:.2f}, Recall: {r_A:.2f}")
print(f"Model B ‚Üí FID: {fid_B:.2f},  IS: {is_B:.2f},  Precision: {p_B:.2f}, Recall: {r_B:.2f}")

print("\n=============== BEST MODEL ===============")
print("BEST FID (lower is better):    ", "A" if fid_A < fid_B else "B")
print("BEST IS (higher is better):    ", "A" if is_A > is_B else "B")
print("BEST PRECISION (higher better):", "A" if p_A > p_B else "B")
print("BEST RECALL (higher is better):", "A" if r_A > r_B else "B")



# @title AI prompt cell

import ipywidgets as widgets
from IPython.display import display, HTML, Markdown,clear_output
from google.colab import ai

dropdown = widgets.Dropdown(
    options=[],
    layout={'width': 'auto'}
)

def update_model_list(new_options):
    dropdown.options = new_options
update_model_list(ai.list_models())

text_input = widgets.Textarea(
    placeholder='Ask me anything....',
    layout={'width': 'auto', 'height': '100px'},
)

button = widgets.Button(
    description='Submit Text',
    disabled=False,
    tooltip='Click to submit the text',
    icon='check'
)

output_area = widgets.Output(
     layout={'width': 'auto', 'max_height': '300px','overflow_y': 'scroll'}
)

def on_button_clicked(b):
    with output_area:
        output_area.clear_output(wait=False)
        accumulated_content = ""
        for new_chunk in ai.generate_text(prompt=text_input.value, model_name=dropdown.value, stream=True):
            if new_chunk is None:
                continue
            accumulated_content += new_chunk
            clear_output(wait=True)
            display(Markdown(accumulated_content))

button.on_click(on_button_clicked)
vbox = widgets.GridBox([dropdown, text_input, button, output_area])

display(HTML("""
<style>
.widget-dropdown select {
    font-size: 18px;
    font-family: "Arial", sans-serif;
}
.widget-textarea textarea {
    font-size: 18px;
    font-family: "Arial", sans-serif;
}
</style>
"""))
display(vbox)



### lab 8


import gradio as gr
from transformers import pipeline

# Load a pre-trained generative model from Hugging Face (GPT-2 for text generation)
generator = pipeline("text-generation", model="gpt2")

def generate_text(prompt):
    result = generator(prompt, max_length=100, num_return_sequences=1)
    return result[0]['generated_text']


# Create a Gradio interface
interface = gr.Interface(
    fn=generate_text,
    inputs=gr.Textbox(lines=2, placeholder="Enter a prompt here..."),
    outputs="text",
    title="Text Generator App",
    description="Generate creative text using Hugging Face GPT-2 model."
)

interface.launch()


